from utils.deep_research_plus_minimal.llm import llm
from langchain.schema import SystemMessage, HumanMessage

def write_report(state):
    # 从状态中获取研究结果
    partial_results = state.get("partial_results", [])
    query = state.get("query", "")
    
    # 构建研究资料文本
    findings_text = ""
    sources_text = ""
    
    for i, result in enumerate(partial_results):
        if isinstance(result, dict) and "claim" in result:
            findings_text += f"\n{i+1}. {result['claim']}\n"
            if "evidence" in result:
                for j, evidence in enumerate(result["evidence"]):
                    sources_text += f"[{i+1}.{j+1}] {evidence.get('title', '')} - {evidence.get('url', '')}\n"

    
    # prompt = f"""
    # # 你是专业教师，请基于以下教学课程，生成一篇可以用来教学的式研究结论。
    # 要求：
    # 1. 按"技术优势 / 应用场景 / 潜在风险"三部分写。
    # 2. 每部分都要有明确结论，避免开放性问题。
    # 3. 用正式中文学术口吻，逻辑清晰，长度 2000-3000 字。
    # 4. 在文中标注引用编号（如 [1]）。
    # 5. 最后附上参考文献列表，对应引用编号。

    # 研究主题：{query}

    # 研究资料：
    # {findings_text}

    # 参考来源：
    # {sources_text}
    # """
    
    querys = query.split(',')
    prompt = f'''
    你是一名经验丰富的{querys[0]}教育专家，擅长该专家的特定技能，例如：将复杂概念转化为易于理解的语言、设计互动性强的学习活动等。
    请基于以下提供的教学元数据，生成一份以“{querys[-1]}”为核心的、详细且结构化的教学知识点内容。

    ## 【背景信息】
    *   **单元名称:** {querys[0]}
    *   **核心知识点:** {querys[1]}
    *   **内容难度:** {querys[2]}
    *   **培养能力项:** {querys[3]}
    *   **本次教学主题:** {querys[4]}

    ## 【核心任务与要求】
    请为上述教学主题生成一份可直接用于教学的内容，需包含以下部分：
    1.  **概念解析**：清晰定义{querys[-1]}的核心概念，详细介绍概念知识点需要具体的阐述其在该领域的意义与作用。
    2.  **知识结构**：梳理与该主题相关的关键子知识点或组成部分，以逻辑清晰的方式（如列表、流程图或思维导图描述）呈现。
    3.  **关键子知识点详解**：清晰定义{querys[1]}的核心概念，详细介绍子知识点需要具体的阐述其在该领域的意义与作用。
    4.  **关键技术优化**：关键技术优化的核心概念，关键技术优化需要具体的阐述其在该领域的意义与作用。
    5.  **优势特点**：介绍相关方法的优势特点，要是没有优势特点可以不写.
    6.  **局限性**：介绍相关方法的局限性，要是没有局限性可以不写.
    7.  **实战参数建议**：如果设计到方法或者代码可以进行实战演练.
    8.  **训练与评估**：对所用到的方法进行评估.
    8.  **总结**：对知识点进行总结.
    9.  **教学应用**：
        *   **真实案例**：结合1-2个行业/领域内的实际案例或场景，说明该知识点的应用价值。
        *   **常见误区**：分析学习或应用此知识点时可能出现的典型错误或理解偏差，并提供辨析方法。
    10.  **学习活动设计**：设计一个针对该知识点的小型学习活动或练习（如小组讨论题、模拟操作步骤、案例分析题等），旨在巩固{querys[3]}。
    11.  **评估与反馈**：提供2-3条用于检验该知识点掌握情况的形成性评价问题或标准。

    ## 【格式与风格约束】
    *   **输出格式**：请使用 Markdown 语法进行结构化排版，合理运用标题、列表、表格等元素，不输出与知识点无关内容。
    *   **语言风格**：语言应专业、准确且简洁，适应目标学员，如：行业新人、大学生等的认知水平。
    *   **内容深度**：内容详实，解释透彻，但避免不必要的冗余信息。

    ## 研究资料：
        {findings_text}

    ## 参考来源：
        {sources_text}
   
    ## 【示例参考，仅供格式参考，知识点内容不需要参考】
    XGBoost 机器学习算法详解

    1 概念解析

    XGBoost（eXtreme Gradient Boosting，极限梯度提升）是一种基于梯度提升决策树（GBDT）的高效机器学习算法，由陈天奇于2014年提出。它通过集成多个弱学习器（通常是决策树）形成一个强学习器，属于Boosting集成学习方法。

    核心概念与意义：
    • 梯度提升：XGBoost采用逐步增强策略，每一棵新树都学习前面所有树的残差，通过不断纠正错误提升模型精度。这与传统的GBDT一阶导数优化不同，XGBoost使用二阶泰勒展开，同时利用一阶和二阶导数信息，使收敛更快更准确。

    • 正则化：XGBoost在目标函数中加入正则项（L1和L2正则化），控制模型复杂度，有效防止过拟合。正则项包含叶子节点个数和叶子节点权重的L2模平方和。

    • 并行处理：虽然树生成是串行的，但XGBoost在特征排序和分裂点选择上实现并行计算，通过预排序和块结构（Block）优化，大幅提升训练速度。

    XGBoost在机器学习领域具有重要意义，因其高效性、准确性和泛化能力，在Kaggle等数据科学竞赛中广泛应用，尤其擅长处理结构化数据问题，如预测房价、用户行为分析等。

    2 知识结构

    XGBoost的知识体系可分为以下几个核心组成部分：

    ├─ 1. 基础理论
    │  ├─ 1.1 梯度提升框架 (Gradient Boosting Framework)
    │  │  └─ 通过迭代添加弱学习器（决策树），每棵新树拟合前一棵树的残差，逐步优化模型。
    │  ├─ 1.2 决策树作为弱学习器 (Decision Trees as Weak Learners)
    │  │  └─ 通常使用CART回归树，因其能有效捕捉非线性关系和特征交互[3]。
    │  └─ 1.3 Boosting串行集成 (Boosting Sequential Ensemble)
    │     └─ 与Bagging并行不同，Boosting是串行过程，新模型依赖于前序模型的结果，重在降低偏差。
    ├─ 2. 目标函数与优化
    │  ├─ 2.1 损失函数 + 正则项 (Loss Function + Regularization Term)
    │  │  └─ 目标函数 = 损失函数（如MSE, LogLoss） + 正则项（L1/L2，控制模型复杂度，防止过拟合）。
    │  ├─ 2.2 二阶泰勒展开 (Second-order Taylor Expansion)
    │  │  └─ 对损失函数进行二阶泰勒展开近似，利用一阶梯度(gi)和二阶梯度(hi)进行更精确的优化，加速收敛。
    │  ├─ 2.3 导数计算: g_i一阶, h_i二阶 (Derivative Calculation)
    │  │  └─ gi是损失函数对当前预测值的一阶导数，hi是二阶导数，提供了梯度变化率的信息[5]。
    │  └─ 2.4 叶子节点权重计算 (Leaf Weight Calculation)
    │     └─ 叶子节点j的最优权重 wj = - (∑gi) / (∑hi + lambda)，此公式由优化正则化后的目标函数推导而来[5]。
    ├─ 3. 算法细节与优化
    │  ├─ 3.1 分裂点查找: 精确贪心算法/近似算法 (Split Finding: Exact Greedy Algorithm / Approximate Algorithm)
    │  │  ├─ 精确贪心算法：遍历所有特征的所有可能分裂点，找到增益最大的分裂点[3]。
    │  │  └─ 近似算法：基于特征分位数提出候选分裂点，适用于大数据集，平衡效率与精度[3]。
    │  ├─ 3.2 并行化: 特征预排序与Block结构 (Parallelization: Feature Pre-sorting & Block Structure)
    │  │  └─ 数据按特征值预排序并存储在Block结构中，允许在特征层面并行计算分裂增益，大幅提升计算效率[3]。
    │  ├─ 3.3 缺失值处理: 自动学习默认方向 (Missing Value Handling: Automatic Default Direction Learning)
    │  │  └─ 算法自动学习对于包含缺失值的特征，应将缺失样本划分到左子树还是右子树才能获得最大增益[3]。
    │  ├─ 3.4 正则化: gamma, lambda, alpha参数 (Regularization: gamma, lambda, alpha Parameters)
    │  │  ├─ gamma：控制分裂所需的最小损失减少量，值越大模型越保守[3]。
    │  │  ├─ lambda：L2正则化系数，对叶子权重进行惩罚[5]。
    │  │  └─ alpha：L1正则化系数，也对叶子权重进行惩罚，可促使权重稀疏[5]。
    │  └─ 3.5 剪枝策略: 后剪枝 (Pruning Strategy: Post-Pruning)
    │     └─ 树构建完成后，从底部向上检查分裂点，如果某个分裂点的增益小于阈值（如gamma），则剪掉该分裂[3]。
    └─ 4. 应用与实践
    ├─ 4.1 二分类/多分类 (Binary Classification / Multi-class Classification)
    │  └─ 使用对数损失（Log Loss）作为损失函数，输出通过Sigmoid（二分类）或Softmax（多分类）转换为概率[5]。
    ├─ 4.2 回归任务 (Regression Tasks)
    │  └─ 常用均方误差（MSE）或平均绝对误差（MAE）作为损失函数[5]。
    ├─ 4.3 排序学习 (Learning to Rank)
    │  └─ 应用于信息检索等领域，优化排序指标[4]。
    └─ 4.4 超参数调优 (Hyperparameter Tuning)
        ├─ 关键参数包括：学习率(eta/learning_rate), 树最大深度(max_depth), 最小子样本权重和(min_child_weight), 行采样(subsample), 列采样(colsample_bytree), 正则化参数(lambda, alpha, gamma)等[3,5]。
        └─ 常用调优方法：网格搜索、随机搜索、贝叶斯优化等，配合交叉验证避免过拟合[4]。


    3 关键子知识点详解：
        1.  目标函数：XGBoost的目标函数由两部分组成：损失函数（衡量预测值与真实值的误差）和正则化项（控制模型复杂度）。正则化项包括叶子节点数的L1正则化和叶子节点权重的L2正则化。
        2.  分裂增益计算：分裂节点的增益通过公式计算，增益越大表示分裂后目标函数下降越多。公式考虑了分裂后左右子树的损失减少以及引入新叶子的复杂度惩罚。
        3.  分裂点查找策略：
            ◦   精确贪心算法：遍历特征所有可能的分裂点，计算增益，选择最优分裂点。计算量大但精确。

            ◦   近似算法：根据特征分布的分位数提出候选分裂点，减少计算量，适用于大规模数据。

        4.  缺失值处理：XGBoost能自动学习缺失值的分裂方向。对于特征值缺失的样本，模型会计算将其划入左子树或右子树带来的增益，选择增益最大的方向作为默认分支。

  
    4 关键技术优化
    4.1  分裂点搜索：
        ◦   使用贪心算法或近似算法（如加权分位略图）高效寻找最佳分裂点，计算分裂增益（Gain）以评估特征重要性。

        ◦   增益公式为：  


    4.2  缺失值处理：自动学习缺失值的分裂方向，计算缺失样本分配至左右子节点的增益，选择增益更大的分支作为默认方向。
    4.3  稀疏数据优化：支持稀疏矩阵格式（如CSR），跳过零特征值计算，提升效率。
    4.4  并行化与硬件加速：
        ◦   特征预排序和分块存储（Block结构），支持多线程并行计算。

        ◦   缓存优化（Cache-aware Access）提高CPU缓存命中率，核外计算处理大规模数据。


    5 优势特点
    5.1  高效性与扩展性：并行计算和分布式训练支持大规模数据集，训练速度比传统GBDT快十倍以上。
    5.2  灵活性：支持自定义损失函数和评估指标，适用于回归、分类、排序任务。
    5.3  鲁棒性：通过正则化、Shrinkage（学习率η）、列抽样（colsample_bytree）等技术防止过拟合。
    5.4  应用广泛：在金融风控、推荐系统、医疗诊断、自然语言处理等领域取得显著成果，尤其在Kaggle等竞赛中屡获佳绩。

    6 局限性
    6.1  计算资源消耗：预排序和内存占用较高，尤其处理超大规模数据时效率低于LightGBM等算法。
    6.2  参数调优复杂：需调整超参数（如学习率、树深度、正则化系数），依赖经验。
    6.3  非结构化数据处理：在图像、音频等非结构化数据上表现不如深度学习模型。

    7 实战参数建议（Python）
    ```python
    import xgboost as xgb
    from sklearn.model_selection import train_test_split

    # 数据准备
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dtest = xgb.DMatrix(X_test, label=y_test)

    # 关键参数设置
    params = 
        'max_depth': 3,           # 树深度，防过拟合
        'eta': 0.1,                # 学习率（Shrinkage）
        'objective': 'binary:logistic',  # 任务目标
        'eval_metric': 'error',          # 评估指标
        'subsample': 0.8,          # 行抽样比例
        'colsample_bytree': 0.8,   # 列抽样比例
        'lambda': 1,               # L2正则化
        'alpha': 0,                # L1正则化
        'gamma': 0,                # 分裂最小增益阈值
        'min_child_weight': 1      # 叶子节点样本权重和下限
    

    8 训练与评估
    model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtest, 'test')])
    predictions = model.predict(dtest)
    ```
    *参数说明详见官方文档，需根据数据特性调整。*

    9 总结
    XGBoost凭借其理论创新（如二阶梯度优化、正则化）和工程优化（并行化、稀疏处理），成为机器学习领域的标杆算法。尽管在超大规模数据上面临挑战，其在高维结构化数据中的表现仍难以替代。建议根据任务需求选择算法，并结合调参工具（如GridSearch）优化性能。
    
    10 教学应用

    10.1 真实案例

    案例一：房价预测（回归问题）
    •   场景描述：利用房屋特征（如面积、房间数量、建造年份、地理位置评分）预测房价。

    •   应用过程：

        1.  初始化模型：用一个简单的预测（如所有房屋价格的平均值）作为起点。
        2.  计算误差：模型检查预测不准的地方（哪些房价被高估或低估）。
        3.  学习误差：XGBoost生成决策树来预测这些误差。决策树根据特征（如先按面积，再按房间数）一步步分类房屋，找到价格规律以修正误差。
        4.  重复修正：不断创建新树修正前序树的误差，最终将所有树的预测结果相加得到准确预测。
    •   价值体现：XGBoost能有效捕捉特征与房价间的复杂关系，处理混合型特征，并通过正则化防止过拟合，从而提供准确的房价预估。

    案例二：金融风控中的欺诈检测（二分类问题）
    •   场景描述：识别信用卡交易是否为欺诈行为。

    •   应用过程：

        1.  数据预处理：处理类别不平衡问题（欺诈交易远少于正常交易），可通过设置scale_pos_weight参数调整。
        2.  模型训练：使用XGBoost的XGBClassifier，目标函数设为binary:logistic，评估指标为AUC。
        3.  预测与评估：模型输出欺诈概率，通过ROC曲线和AUC值评估模型性能。
    •   价值体现：XGBoost能高效处理高维特征，自动处理缺失值，并通过正则化避免过拟合，在保证高准确率的同时提升欺诈检测效率。

    10.2 常见误区与辨析

    1.  误区一：XGBoost在任何数据集上都比深度学习好
        ◦   辨析：并非如此。XGBoost更擅长处理结构化/表格数据。对于非结构化数据（如图像、音频、文本），深度学习（如CNN、RNN）通常表现更好。选择模型需依据数据特点和问题类型。

    2.  误区二：XGBoost参数越多越好，调参越复杂模型效果越好
        ◦   辨析：参数多虽提供了高灵活性，但也增加了过拟合和调参难度。关键参数（如learning_rate, max_depth, subsample, colsample_bytree, reg_lambda）需要仔细调整，但并非所有参数都需极致优化。通常建议先进行网格搜索或随机搜索重点参数，并结合交叉验证评估。

    3.  误区三：XGBoost不需要预处理缺失值
        ◦   辨析：XGBoost能自动学习缺失值的处理方式，这是其一大优势。但这并不意味着无需关注缺失值。大量缺失可能意味着特征重要性低，甚至引入噪声。理解缺失机制、判断特征重要性，有时手动处理（如填充或删除）可能效果更好。

    4.  误区四：XGBoost因为并行所以训练很快
        ◦   辨析：XGBoost的Boosting过程本身是串行的，每棵树需依赖前一棵树的结果。其并行主要体现在特征层面，如预排序和分裂点计算。在大数据集上，其速度优势源于算法优化（如加权分位数草图、缓存感知等），而非整个树的构建过程并行。

    11 学习活动设计

    活动名称： XGBoost房价预测实战

    活动目标： 巩固对XGBoost原理的理解，掌握其在实际回归任务中的应用流程，包括数据准备、模型训练、评估和参数调优。

    活动内容与步骤：
    1.  数据探索与预处理 (15分钟)
        ◦   提供一份房屋数据集（包含面积、房龄、房间数、位置评分、价格等特征）。

        ◦   学员使用Python的Pandas库进行数据探索，检查缺失值、特征分布等。

        ◦   进行特征工程，如处理缺失值（可体验XGBoost自动处理与手动处理的差异）、数值型特征标准化等。

    2.  模型训练与预测 (20分钟)
        ◦   将数据划分为训练集和测试集。

        ◦   使用xgboost库的XGBRegressor初始化模型，设置基本参数（如objective='reg:squarederror', eval_metric='rmse'）。

        ◦   在训练集上训练模型，并在测试集上进行预测。

    3.  性能评估与调参 (25分钟)
        ◦   计算预测结果的RMSE（均方根误差）和MAE（平均绝对误差）。

        ◦   绘制特征重要性图谱，分析哪些特征对房价预测贡献最大。

        ◦   尝试调整2-3个关键超参数（如max_depth, learning_rate, n_estimators），观察模型性能变化，思考参数对偏差和方差的影响。

    4.  小组讨论与汇报 (20分钟)
        ◦   小组讨论以下问题：

            ▪   在调整参数过程中，哪些参数对模型性能影响最显著？为什么？

            ▪   特征重要性结果是否符合你们的先验认知？如何解释？

            ▪   对比XGBoost和线性回归在这个任务上的表现，你认为XGBoost的优势在哪里？

        ◦   每组选派代表简要分享实验结果和讨论 insights。

    所需工具/资源：
    •   Python环境 (Jupyter Notebook)

    •   库: xgboost, pandas, numpy, scikit-learn, matplotlib

    •   数据集: 加州房价数据集或类似的公开数据集

    12 评估与反馈

    1.  形成性评价问题一：解释XGBoost目标函数中正则化项的作用。
        ◦   评估标准：

            ▪   优秀：能准确说明正则化项包含叶子节点数（L1）和叶子节点权重L2范数（L2），并解释其通过惩罚复杂模型来控制过拟合的原理。

            ▪   合格：能提到正则化用于防止过拟合，但可能混淆L1和L2的具体形式或作用。

            ▪   待提高：无法清楚解释正则化项的含义或作用。

    2.  形成性评价问题二：描述XGBoost如何处理特征缺失的情况。
        ◦   评估标准：

            ▪   优秀：能说明XGBoost通过计算将缺失值样本分配到左子树或右子树所带来的增益，自动学习最优的默认分裂方向，无需人工填充。

            ▪   合格：知道XGBoost能自动处理缺失值，但细节描述不清晰。

            ▪   待提高：认为XGBoost无法处理缺失值或需要手动预处理。

    3.  形成性评价问题三：在调整XGBoost超参数时，如果模型在训练集上表现很好但在测试集上表现很差，可能是什么原因？你会如何调整参数来尝试改善？
        ◦   评估标准：

            ▪   优秀：能判断这是过拟合现象，并提出针对性的参数调整策略，如增大正则化参数（reg_alpha, reg_lambda）、降低模型复杂度（减小max_depth、增加min_child_weight）、减小learning_rate并增加n_estimators、引入随机性（减小subsample或colsample_bytree）。

            ▪   合格：能识别出过拟合，但提出的参数调整策略可能较单一或不够准确。

            ▪   待提高：无法将现象与过拟合联系起来，或提出的调整方向错误（如继续增大模型复杂度）。
    '''
    
    
    msg = llm.invoke([SystemMessage(content="你是专业领域教学员"), HumanMessage(content=prompt)])
    return {"final_report_md": msg.content}